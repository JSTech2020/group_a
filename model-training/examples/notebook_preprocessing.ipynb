{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import sys\n",
    "import re\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters and variable init for db operations\n",
    "uri = 'mongodb://localhost:27017/'\n",
    "database = 'zs_database'\n",
    "collection_fetch = 'stories'\n",
    "collection_push = 'autotags'\n",
    "\n",
    "df = pd.DataFrame()\n",
    "db = object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "get unprocessed data into pandas dataframe\n",
    "\"\"\"\n",
    "# exception handling for database operation\n",
    "try:\n",
    "    print(\"Connecting to database\")\n",
    "    # init mongodb client\n",
    "    client = MongoClient(uri)\n",
    "    db = client[database]\n",
    "\n",
    "    # retrieving only story id and details for now\n",
    "    df = pd.DataFrame(list(db[collection_fetch].find({}, {\"_id\":0, \"id\": 1, \"plain_text\": 1})))\n",
    "\n",
    "except:  # TODO: maybe be specific about the exceptions that can occur\n",
    "    print('Unexpected error:', sys.exc_info()[0])\n",
    "    print('Exiting system ...')\n",
    "    exit()\n",
    "    \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Clean given text\n",
    "\"\"\"\n",
    "def clean_text(content):\n",
    "    \"\"\"\n",
    "    Removing Unwanted Characters\n",
    "    \"\"\"\n",
    "    # removing html tags\n",
    "    content = re.sub('<[^<]+?>', '', content)\n",
    "\n",
    "    # removing entity names\n",
    "    content = re.sub('&[^<]+?;', '', content)\n",
    "\n",
    "    # removing whitespace from escape characters\n",
    "    content = re.sub(r'[\\n\\r\\t\\a\\f\\b\\v]', '', content)\n",
    "\n",
    "    # remove unwanted characters (eg \",',.,?,!). We might want to kee these later though.\n",
    "    content = re.sub(r'[\\'\\-\".?!,0-9“„–()]', '', content)\n",
    "\n",
    "    \"\"\"\n",
    "    Encoding the proper format\n",
    "    \"\"\"\n",
    "    # python 3 handles sting in UTF-8 by default\n",
    "    # We need to write ode if we want to process data in other formats\n",
    "    # for now default UTF-8 is ok\n",
    "    \n",
    "    return content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generate tokens for given text\n",
    "\"\"\"\n",
    "def generate_tokens(content):   \n",
    "    #load plain text into spacy processor     \n",
    "    doc = nlp(content)\n",
    "\n",
    "    # init list\n",
    "    token_list = []\n",
    "    lemma_list = []\n",
    "    pos_list = []\n",
    "    \n",
    "    # Iterate through each token identified in doc    \n",
    "    for token in doc:\n",
    "        # remove stop words for German Language like like 'eine', 'könnte'... from spacy lib.\n",
    "        if (not token.is_stop) and (token.text != \" \") and (token.text != \"\"):\n",
    "            # additional trimming needed for some cases\n",
    "            word = token.text.strip()\n",
    "            lemma = token.lemma_.strip()\n",
    "            pos = token.pos_.strip()\n",
    "\n",
    "            # addictionl check to see if the trimmed or converted text is not empty\n",
    "            if word != '':\n",
    "                token_list.append(word)  # token list without stop word\n",
    "            if lemma != '':\n",
    "                lemma_list.append(lemma)\n",
    "            if pos != '':\n",
    "                pos_list.append(pos)\n",
    "\n",
    "    # Entity listing through spaCy lib. requires text without stop words for wfficiency\n",
    "    entity_list = [[i.text, i.label_] for i in doc.ents]\n",
    "    noun_list = [chunk.text for chunk in doc.noun_chunks]\n",
    "    \n",
    "    return (token_list, lemma_list, pos_list, entity_list, noun_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "preprocess text and insert into collection\n",
    "\"\"\"\n",
    "\n",
    "# load spacy core for German language\n",
    "nlp = spacy.load('de_core_news_sm')\n",
    "\n",
    "# check if push collection(autotags) already exists, if so, remove(drop) the collection for now\n",
    "# TODO: handle exception\n",
    "if collection_push in db.list_collection_names():\n",
    "    collection = db[collection_push]\n",
    "    if collection.estimated_document_count() != 0:\n",
    "        print('Dropping the old collection (' + collection_push + ') ...')\n",
    "        collection.drop()\n",
    "            \n",
    "collection = db[collection_push]\n",
    "\n",
    "print(\"Pre-processing all text. This might take some time...\")\n",
    "print(\"Story id(s) processed: \", end=\" \")\n",
    "for x in df.iterrows():\n",
    "    # fetching id and content for each item in data-frame\n",
    "    index, item = x\n",
    "    story_id = item.id\n",
    "    content = item.plain_text\n",
    "\n",
    "    # clean text for each document \n",
    "    content = clean_text(content)\n",
    "\n",
    "    \"\"\"\n",
    "    Creating Tokens without stop words\n",
    "    Create lemma list and part of speech list in case we need it later\n",
    "    \"\"\"\n",
    "    # word tokanization and other preprocessing for each document\n",
    "    token_list, lemma_list, pos_list, entity_list, noun_list = generate_tokens(content)\n",
    "\n",
    "    # TODO\n",
    "    # if we are going to use un-cased data sets, we need to change the tokens to lower case\n",
    "    # use spaCy sentencizer component if sentence tokenizing is needed\n",
    "\n",
    "    # insert into db\n",
    "    # TODO: write try catch statement, possibly ignore this if singular document is not inserted and continue with other\n",
    "    collection.insert_one(\n",
    "        {\n",
    "            \"story_id\": story_id,\n",
    "            \"tokens\": token_list,\n",
    "            \"lemmas\": lemma_list,\n",
    "            \"pos\": pos_list,\n",
    "            \"nouns\": noun_list,\n",
    "            \"entities\": entity_list\n",
    "        }\n",
    "    )\n",
    "    print(str(story_id), end=\" \")\n",
    "\n",
    "print('Done !!!')\n",
    "print('Pre-processed data entered into (' + collection_push + ') collection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import sys\n",
    "import re\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters and variable init for db operations\n",
    "uri = 'mongodb://localhost:27017/'\n",
    "database = 'zs_database'\n",
    "collection_fetch = 'stories'\n",
    "collection_push = 'autotags'\n",
    "\n",
    "df = pd.DataFrame()\n",
    "db = object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to database\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>plain_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>Radio Über-All sendet 6 mal Zukunftsmusik! Am ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21</td>\n",
       "      <td>Unser Dorf pflanzt SauerstoffMathilda schaute ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24</td>\n",
       "      <td>Es war einmal vor gar nicht so langer Zeit, da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25</td>\n",
       "      <td>Das Beste an Partys ist das Buffet, findet Ber...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27</td>\n",
       "      <td>Endlose Laubwälder, saftige Weiden, Wildblumen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>489</td>\n",
       "      <td>Familie Maus verläßt ihr gutes, altes HausErst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>490</td>\n",
       "      <td>Das Wolkenland Glück war einst ein Merkmal des...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>491</td>\n",
       "      <td>Eines Tages wacht Miro auf, und findet eine ko...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>492</td>\n",
       "      <td>Goldene Sonnenstrahlen fielen durch die klare ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>494</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                         plain_text\n",
       "0     19  Radio Über-All sendet 6 mal Zukunftsmusik! Am ...\n",
       "1     21  Unser Dorf pflanzt SauerstoffMathilda schaute ...\n",
       "2     24  Es war einmal vor gar nicht so langer Zeit, da...\n",
       "3     25  Das Beste an Partys ist das Buffet, findet Ber...\n",
       "4     27  Endlose Laubwälder, saftige Weiden, Wildblumen...\n",
       "..   ...                                                ...\n",
       "195  489  Familie Maus verläßt ihr gutes, altes HausErst...\n",
       "196  490  Das Wolkenland Glück war einst ein Merkmal des...\n",
       "197  491  Eines Tages wacht Miro auf, und findet eine ko...\n",
       "198  492  Goldene Sonnenstrahlen fielen durch die klare ...\n",
       "199  494                                                ...\n",
       "\n",
       "[200 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "get unprocessed data into pandas dataframe\n",
    "\"\"\"\n",
    "# exception handling for database operation\n",
    "try:\n",
    "    print(\"Connecting to database\")\n",
    "    # init mongodb client\n",
    "    client = MongoClient(uri)\n",
    "    db = client[database]\n",
    "\n",
    "    # retrieving only story id and details for now\n",
    "    df = pd.DataFrame(list(db[collection_fetch].find({}, {\"_id\":0, \"id\": 1, \"plain_text\": 1})))\n",
    "\n",
    "except:  # TODO: maybe be specific about the exceptions that can occur\n",
    "    print('Unexpected error:', sys.exc_info()[0])\n",
    "    print('Exiting system ...')\n",
    "    exit()\n",
    "    \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Clean given text\n",
    "\"\"\"\n",
    "def clean_text(content):\n",
    "    \"\"\"\n",
    "    Removing Unwanted Characters\n",
    "    \"\"\"\n",
    "    # removing html tags\n",
    "    content = re.sub('<[^<]+?>', '', content)\n",
    "\n",
    "    # removing entity names\n",
    "    content = re.sub('&[^<]+?;', '', content)\n",
    "\n",
    "    # removing whitespace from escape characters\n",
    "    content = re.sub(r'[\\n\\r\\t\\a\\f\\b\\v]', '', content)\n",
    "\n",
    "    # remove unwanted characters (eg \",',.,?,!). We might want to kee these later though.\n",
    "    content = re.sub(r'[\\'\\-\".?!,0-9“„–()]', '', content)\n",
    "    content = re.sub(r'[\\\\\\\".?!,0-9():;]', '', content)\n",
    "    # remove additional characters\n",
    "    content = re.sub(r'[\\‘\\'\\-\\[\\]»«0-9“„”…–]', '', content)\n",
    "\n",
    "    \"\"\"\n",
    "    Encoding the proper format\n",
    "    \"\"\"\n",
    "    # python 3 handles sting in UTF-8 by default\n",
    "    # We need to write ode if we want to process data in other formats\n",
    "    # for now default UTF-8 is ok\n",
    "    \n",
    "    return content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generate tokens for given text\n",
    "\"\"\"\n",
    "def generate_tokens(content):   \n",
    "    #load plain text into spacy processor     \n",
    "    doc = nlp(content)\n",
    "\n",
    "    # init list\n",
    "    token_list = []\n",
    "    lemma_list = []\n",
    "    lemma_list_without_verbs = []\n",
    "    pos_list = []\n",
    "    \n",
    "    # Iterate through each token identified in doc    \n",
    "    for token in doc:\n",
    "        # remove stop words for German Language like like 'eine', 'könnte'... from spacy lib.\n",
    "        if (not token.is_stop) and (token.text != \" \") and (token.text.strip() != \"\"):\n",
    "            # additional trimming needed for some cases\n",
    "            word = token.text.strip()\n",
    "            lemma = token.lemma_.strip()\n",
    "            pos = token.pos_.strip()\n",
    "\n",
    "            # addictionl check to see if the trimmed or converted text is not empty\n",
    "            if word != '':\n",
    "                token_list.append(word)  # token list without stop word\n",
    "            if lemma != '':\n",
    "                lemma_list.append(lemma)\n",
    "                if pos != \"VERB\" and pos != \"ADV\":\n",
    "                    lemma_list_without_verbs.append(lemma)\n",
    "            if pos != '':\n",
    "                pos_list.append(pos)\n",
    "\n",
    "    # Entity listing through spaCy lib. requires text without stop words for wfficiency\n",
    "    entity_list = [[i.text, i.label_] for i in doc.ents]\n",
    "    noun_list = [chunk.text for chunk in doc.noun_chunks]\n",
    "    \n",
    "    return (token_list, lemma_list, pos_list, entity_list, noun_list, lemma_list_without_verbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing all text. This might take some time...\n",
      "Story id(s) processed:  19 21 24 25 27 28 64 68 69 72 74 94 100 105 107 110 114 118 119 121 124 125 131 132 138 139 140 142 177 180 185 187 188 189 194 198 199 201 202 203 207 214 215 216 217 219 224 225 226 228 233 237 248 251 252 255 256 258 260 262 266 269 272 273 274 281 286 288 289 291 292 293 294 296 297 299 300 302 305 306 307 308 310 315 316 319 320 321 323 325 326 329 330 331 332 333 334 336 340 341 344 347 348 350 353 358 359 360 361 362 364 365 366 367 369 370 371 374 375 378 379 380 381 382 383 384 387 390 391 392 393 394 396 398 399 400 401 403 406 407 408 409 410 411 412 414 416 417 418 419 421 423 424 426 427 428 429 432 433 436 438 439 440 442 443 444 445 446 448 450 451 452 453 454 456 457 458 459 460 462 463 464 469 471 472 473 474 475 478 479 480 485 486 487 488 489 490 491 492 494 Done !!!\n",
      "Pre-processed data entered into (autotags) collection\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "preprocess text and insert into collection\n",
    "\"\"\"\n",
    "\n",
    "# load spacy core for German language\n",
    "nlp = spacy.load('de_core_news_md')\n",
    "\n",
    "# check if push collection(autotags) already exists, if so, remove(drop) the collection for now\n",
    "# TODO: handle exception\n",
    "if collection_push in db.list_collection_names():\n",
    "    collection = db[collection_push]\n",
    "    if collection.estimated_document_count() != 0:\n",
    "        print('Dropping the old collection (' + collection_push + ') ...')\n",
    "        collection.drop()\n",
    "            \n",
    "collection = db[collection_push]\n",
    "\n",
    "print(\"Pre-processing all text. This might take some time...\")\n",
    "print(\"Story id(s) processed: \", end=\" \")\n",
    "for x in df.iterrows():\n",
    "    # fetching id and content for each item in data-frame\n",
    "    index, item = x\n",
    "    story_id = item.id\n",
    "    content = item.plain_text\n",
    "\n",
    "    # clean text for each document \n",
    "    content = clean_text(content)\n",
    "\n",
    "    \"\"\"\n",
    "    Creating Tokens without stop words\n",
    "    Create lemma list and part of speech list in case we need it later\n",
    "    \"\"\"\n",
    "    # word tokanization and other preprocessing for each document\n",
    "    token_list, lemma_list, pos_list, entity_list, noun_list, lemma_list_without_verbs = generate_tokens(content)\n",
    "\n",
    "    # TODO\n",
    "    # if we are going to use un-cased data sets, we need to change the tokens to lower case\n",
    "    # use spaCy sentencizer component if sentence tokenizing is needed\n",
    "\n",
    "    # insert into db\n",
    "    # TODO: write try catch statement, possibly ignore this if singular document is not inserted and continue with other\n",
    "    collection.insert_one(\n",
    "        {\n",
    "            \"story_id\": story_id,\n",
    "            \"tokens\": token_list,\n",
    "            \"lemmas\": lemma_list,\n",
    "            \"pos\": pos_list,\n",
    "            \"nouns\": noun_list,\n",
    "            \"entities\": entity_list,\n",
    "            \"lemma_list_without_verbs\": lemma_list_without_verbs\n",
    "        }\n",
    "    )\n",
    "    print(str(story_id), end=\" \")\n",
    "\n",
    "print('Done !!!')\n",
    "print('Pre-processed data entered into (' + collection_push + ') collection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
